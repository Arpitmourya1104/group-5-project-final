name: S3 to S3 Transfer

on:
  push:
    branches:
      - main

jobs:
  transfer:
    runs-on: ubuntu-latest

    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_SESSION_TOKEN: ${{ secrets.AWS_SESSION_TOKEN }}
      AWS_DEFAULT_REGION: 'us-east-1'

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'

    - name: Set up Spark
      uses: databricks/cli-action@v1.0.4

    - name: Install dependencies
      run: pip install pyspark boto3

    - name: Run PySpark script
      env:
        PYSPARK_SUBMIT_ARGS: "--packages org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.271 pyspark-shell"
      run: python s3_transfer.py
